import streamlit as st
from rag_guardrail import RAGGuardrail
from dataset_qa import process_query as dataset_process_query
import pandas as pd
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="RAG Guardrail é–‹æ”¾è³‡æ–™å°è©±ç³»çµ±",
    page_icon="ğŸ¤–",
    layout="wide"
)

# åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_suggestions" not in st.session_state:
    st.session_state.current_suggestions = {}

def display_chat_history():
    """é¡¯ç¤ºå°è©±æ­·å²"""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

def display_suggestions(suggestions):
    """é¡¯ç¤ºå»ºè­°æŒ‰éˆ•"""
    if suggestions:
        st.write("æ‚¨å¯ä»¥é»æ“Šä»¥ä¸‹å»ºè­°ç¹¼çºŒå°è©±ï¼š")
        cols = st.columns(len(suggestions))
        for idx, (key, suggestion) in enumerate(suggestions.items()):
            with cols[idx]:
                if st.button(suggestion, key=f"suggestion_{key}"):
                    return suggestion
    return None

def process_ai_response(prompt, rag_guardrail):
    """è™•ç† AI éŸ¿æ‡‰"""
    # æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯åˆ°æ­·å²
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # é¡¯ç¤ºç”¨æˆ¶æ¶ˆæ¯
    with st.chat_message("user"):
        st.write(prompt)
    
    # ç²å– AI éŸ¿æ‡‰
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            gl_state = rag_guardrail.process_query(prompt)
            if not gl_state["is_allowed"]:
                AI_feedback = "æ²’æœ‰æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„ç­”æ¡ˆï¼Œä»¥ä¸‹æ˜¯ä¸€äº›ç›¸é—œçš„å•é¡Œå»ºè­°ï¼š"
                st.warning(AI_feedback)
                st.session_state.current_suggestions = gl_state["suggestions"]
            else:
                # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„è³‡æ–™é›†ï¼Œä½¿ç”¨ dataset_qa è™•ç†æŸ¥è©¢
                if gl_state["target_dataset"] != "å…¶ä»–":
                    try:
                        dataset_info = {
                            "è³‡æ–™é›†åç¨±": gl_state["target_dataset"].metadata.get("è³‡æ–™é›†åç¨±", ""),
                            "è³‡æ–™é›†æè¿°": gl_state["target_dataset"].metadata.get("è³‡æ–™é›†æè¿°", ""),
                            "è³‡æ–™ä¸‹è¼‰ç¶²å€": gl_state["target_dataset"].metadata.get("è³‡æ–™ä¸‹è¼‰ç¶²å€", ""),
                            "ä¸»è¦æ¬„ä½èªªæ˜": gl_state["target_dataset"].metadata.get("ä¸»è¦æ¬„ä½èªªæ˜", "")
                        }
                        AI_feedback = dataset_process_query(dataset_info, prompt)
                    except Exception as e:
                        AI_feedback = f"è™•ç†æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
                else:
                    AI_feedback = gl_state["intention"]
                st.session_state.current_suggestions = {}
                st.write(AI_feedback)
    
    # æ·»åŠ  AI éŸ¿æ‡‰åˆ°æ­·å²
    st.session_state.messages.append({"role": "assistant", "content": AI_feedback})

def main():
    rag_guardrail = RAGGuardrail()
    st.title("ğŸ¤– å°è©±ç³»çµ±")

    # ä¸»ç•Œé¢ - å°è©±å€åŸŸ
    st.header("ğŸ’¬ é–‹æ”¾è³‡æ–™å°è©±æ©Ÿå™¨äºº")
    
    # é¡¯ç¤ºå°è©±æ­·å²
    display_chat_history()
    
    # é¡¯ç¤ºå»ºè­°æŒ‰éˆ•
    if st.session_state.current_suggestions:
        selected_suggestion = display_suggestions(st.session_state.current_suggestions)
        if selected_suggestion:
            process_ai_response(selected_suggestion, rag_guardrail)
            st.rerun()
    
    # ç”¨æˆ¶è¼¸å…¥
    if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ"):
        process_ai_response(prompt, rag_guardrail)
        st.rerun()
    
    # æ·»åŠ æ¸…é™¤å°è©±æŒ‰éˆ•
    if st.button("æ¸…é™¤å°è©±æ­·å²"):
        st.session_state.messages = []
        st.session_state.current_suggestions = {}
        st.rerun()

if __name__ == "__main__":
    main() 